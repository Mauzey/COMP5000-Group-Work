{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_columns', 35)"
   ]
  },
  {
   "source": [
    "# Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = pd.read_csv(\"../data/raw/customers.csv\")\n",
    "vendors_df = pd.read_csv(\"../data/raw/vendors.csv\")\n",
    "locations_df = pd.read_csv(\"../data/raw/locations.csv\")\n",
    "orders_df = pd.read_csv(\"../data/raw/orders.csv\")"
   ]
  },
  {
   "source": [
    "# Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Drop Duplicate Customer Observations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = customers_df.drop_duplicates(subset=['akeed_customer_id'], keep='first')"
   ]
  },
  {
   "source": [
    "## Drop Vendor Columns Specified in the Assessment Brief"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_df = vendors_df.drop([\n",
    "    'sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2', 'sunday_to_time2', 'monday_from_time1',\n",
    "    'monday_to_time1', 'monday_from_time2', 'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1',\n",
    "    'tuesday_from_time2', 'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2',\n",
    "    'friday_from_time1', 'friday_to_time1', 'friday_from_time2', 'friday_to_time2', 'saturday_from_time1',\n",
    "    'saturday_to_time1', 'saturday_from_time2', 'saturday_to_time2'\n",
    "], axis=1)"
   ]
  },
  {
   "source": [
    "## Drop Orders with Empty `customer_id` or `item_count` Fields"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df['customer_id'].replace('', np.nan, inplace=True)\n",
    "orders_df['item_count'].replace('', np.nan, inplace=True)\n",
    "orders_df = orders_df.dropna(subset=['customer_id', 'item_count'])"
   ]
  },
  {
   "source": [
    "# Refactoring the Indexing System\n",
    "## Rename Legacy `ID` Columns\n",
    "We'll be creating new ID columns which use incremental integers. This new system will be the same across all tables; the current system uses a number of different indexing methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = customers_df.rename(columns={'akeed_customer_id': 'legacy_customer_id'})\n",
    "vendors_df = vendors_df.rename(columns={'id': 'legacy_vendor_id'})\n",
    "locations_df = locations_df.rename(columns={'customer_id': 'legacy_customer_id'})\n",
    "orders_df = orders_df.rename(columns={\n",
    "    'akeed_order_id': 'legacy_order_id', 'customer_id': 'legacy_customer_id', 'vendor_id': 'legacy_vendor_id'\n",
    "})"
   ]
  },
  {
   "source": [
    "## Create a New `customer_id`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df['customer_id'] = customers_df.index"
   ]
  },
  {
   "source": [
    "### Copy the new `customer_id` to the other dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of 'legacy_customer_id' and the corresponding 'customer_id'\n",
    "customer_ids = customers_df.set_index('legacy_customer_id')['customer_id'].to_dict()\n",
    "\n",
    "# copy the new 'customer_id' values to 'locations_df' and 'orders_df'\n",
    "locations_df['customer_id'] = locations_df['legacy_customer_id'].map(customer_ids)\n",
    "orders_df['customer_id'] = orders_df['legacy_customer_id'].map(customer_ids)\n",
    "\n",
    "# drop 'legacy_customer_id' from 'customers_df', 'locations_df', and 'orders_df'\n",
    "customers_df = customers_df.drop(columns=['legacy_customer_id'], axis=1)\n",
    "locations_df = locations_df.drop(columns=['legacy_customer_id'], axis=1)\n",
    "orders_df = orders_df.drop(columns=['legacy_customer_id'], axis=1)"
   ]
  },
  {
   "source": [
    "We can now see that there are a number of missing `customer_id` values in `locations_df` and `orders_df`; this is because certain locations/orders pertain to customers which once existed in the dataset, but no longer do:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(\"Missing 'customer_id' count in 'locations_df': \", locations_df['customer_id'].isnull().sum())\n",
    "print(\"Missing 'customer_id' count in 'orders_df': \", orders_df['customer_id'].isnull().sum())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Missing 'customer_id' count in 'locations_df':  1479\nMissing 'customer_id' count in 'orders_df':  3101\n"
     ]
    }
   ]
  },
  {
   "source": [
    "...we can drop these records:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = locations_df.dropna(subset=['customer_id'])\n",
    "orders_df = orders_df.dropna(subset=['customer_id'])"
   ]
  },
  {
   "source": [
    "## Create a New `vendor_id`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_df['vendor_id'] = vendors_df.index"
   ]
  },
  {
   "source": [
    "### Copy the new `vendor_id` to the other dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of 'legacy_vendor_id' and the corresponding 'vendor_id'\n",
    "vendor_ids = vendors_df.set_index('legacy_vendor_id')['vendor_id'].to_dict()\n",
    "\n",
    "# copy the new 'vendor_id' to 'orders_df'\n",
    "orders_df['vendor_id'] = orders_df['legacy_vendor_id'].map(vendor_ids)\n",
    "\n",
    "# drop 'legacy_vendor_id' from 'vendors_df' and 'orders_df'\n",
    "vendors_df = vendors_df.drop(columns=['legacy_vendor_id'], axis=1)\n",
    "orders_df = orders_df.drop(columns=['legacy_vendor_id'], axis=1)"
   ]
  },
  {
   "source": [
    "## Create a New `order_id`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df['order_id'] = orders_df.index\n",
    "\n",
    "# drop 'legacy_order_id' from 'orders_df'\n",
    "orders_df = orders_df.drop(columns=['legacy_order_id'], axis=1)"
   ]
  },
  {
   "source": [
    "## Create a New `location_id`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df['location_id'] = locations_df.index"
   ]
  },
  {
   "source": [
    "### Copy the new `location_id` to the other dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_location_id(customer_id, location_number):\n",
    "    return locations_df.loc[\n",
    "        (locations_df['customer_id'] == customer_id) & (locations_df['location_number'] == location_number)\n",
    "    ]['location_id'].values[0]\n",
    "\n",
    "orders_df['location_id'] = [update_location_id(*a) for a in tuple(\n",
    "    zip(orders_df['customer_id'], orders_df['LOCATION_NUMBER'])\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'location_number' from 'locations_df'\n",
    "locations_df = locations_df.drop(columns=['location_number'])\n",
    "\n",
    "# drop 'LOCATION_NUMBER' and 'LOCATION_TYPE' from 'orders_df'\n",
    "orders_df = orders_df.drop(columns=['LOCATION_NUMBER', 'LOCATION_TYPE'])"
   ]
  },
  {
   "source": [
    "# More Data Cleaning\n",
    "Now that we've refactored the ID system, we can continue with the data cleaning.\n",
    "\n",
    "## Customer Data\n",
    "There doesn't seem to be much to do here. It's worth noting that there is a significant amount of missing values in `gender`, `dob`, and `language`. In order to fix this, the company would have to reach out to the customers in question and ask them to update their details."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "gender         12154\n",
       "dob            31477\n",
       "status             0\n",
       "verified           0\n",
       "language       13424\n",
       "created_at         0\n",
       "updated_at         0\n",
       "customer_id        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "customers_df.isnull().sum()"
   ]
  },
  {
   "source": [
    "We can rename `dob`, as it holds customers' year of birth rather than a specific 'date of birth':"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = customers_df.rename(columns={'dob': 'birth_year'})"
   ]
  },
  {
   "source": [
    "## Vendor Data\n",
    "`authentication_id` is a float, however it contains integer-like values; let's fix this:\n",
    "\n",
    "_There are a number of columns which also share this issue, however they will be removed later on so it would be pointless changing them here._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_df['authentication_id'] = vendors_df['authentication_id'].astype(int)"
   ]
  },
  {
   "source": [
    "We can drop the `primary_tags` column as it doesn't seem to correspond with any other data we have here. It also invalidates the vendor table's second normal form.\n",
    "\n",
    "_`vendor_tag` and `vendor_tag_name` also invalidates the vendor table's 2NF, however we'll be extracting these values into their own tables shortly._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_df = vendors_df.drop(columns=['primary_tags'], axis=1)"
   ]
  },
  {
   "source": [
    "The `is_open` column can be dropped as it seems redundant - whether a vendor is open or not can be determined on the front end by checking the opening time columns. The same reasoning can be applied to the removal of `open_close_flags`.\n",
    "\n",
    "The `country_id` and `city_id` columns seem to reference tables which aren't present in our data, so they can be dropped as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_df = vendors_df.drop(columns=['is_open', 'open_close_flags', 'country_id', 'city_id'], axis=1)"
   ]
  },
  {
   "source": [
    "Finally, let's rename some of the columns for the sake of readability:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_df = vendors_df.rename(columns={\n",
    "    'authentication_id': 'auth_id', 'delivery_charge': 'delivery_fee', 'serving_distance': 'max_serving_dist',\n",
    "    'OpeningTime': 'opening_time', 'OpeningTime2': 'opening_time_2', 'prepration_time': 'preparation_time',\n",
    "    'vendor_rating': 'avg_vendor_rating', 'one_click_vendor': 'is_one_click'\n",
    "})"
   ]
  },
  {
   "source": [
    "Before we continue, it's worth noting that the `commission`, `language`, `vendor_tag`, and `vendor_tag_name` columns contain missing values. These would need to be corrected in collaboration with the vendors in question.\n",
    "\n",
    "## Location Data\n",
    "There isn't much to do here. `customer_id` is of type float, when it should be int - let's fix this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df['customer_id'] = locations_df['customer_id'].astype(int)"
   ]
  },
  {
   "source": [
    "## Order Data\n",
    "Let's start by correcting the `item_count` dtype:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df['item_count'] = orders_df['item_count'].astype(int)"
   ]
  },
  {
   "source": [
    "Next, the `delivery_time` column doesn't appear to be providing any value as very few columns have a valid entry in this field, so we can drop it:\n",
    "\n",
    "_The same can be said for `promo_code`, however we'll keep this column and move it to it's own table for the sake of example._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of records in 'orders' table:  125277\nMissing values in 'delivery_time':  120569\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of records in 'orders' table: \", orders_df.shape[0])\n",
    "print(\"Missing values in 'delivery_time': \", orders_df['delivery_time'].isnull().sum())\n",
    "\n",
    "orders_df = orders_df.drop(columns=['delivery_time'], axis=1)"
   ]
  },
  {
   "source": [
    "`driver_rating` pertains to the delivery driver assigned to the order. As we don't have a table listing driver information, we can drop this column:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_df.drop(columns=['driver_rating'], axis=1)"
   ]
  },
  {
   "source": [
    "The `CID X LOC_NUM X VENDOR` column is useless in the context of this assignment as it's used as a submission for the Kaggle challenge from which this dataset originates. Let's drop it:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_df.drop(columns=['CID X LOC_NUM X VENDOR'], axis=1)"
   ]
  },
  {
   "source": [
    "Finally, let's fix the `customer_id` dtype:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df['customer_id'] = orders_df['customer_id'].astype(int)"
   ]
  },
  {
   "source": [
    "# Extracting Vendor Category Information\n",
    "Despite all of the vendors in our dataset having a single category, we can extract `vendor_category_en` into its own table. This would allow for a vendor to have multiple categories in the future which would enhance the user search experience. (_Some vendors might be a restaurant which also sells sweets & bakes, for example_)\n",
    "\n",
    "We must also create a second table to house the relationships between vendors and their categories.\n",
    "\n",
    "### Create a table which contains each category and an identifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_categories_df = vendors_df[['vendor_category_en']].copy().rename(\n",
    "    columns={'vendor_category_en': 'category'}\n",
    ").drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "# use the pandas index as 'category_id'\n",
    "vendor_categories_df['category_id'] = vendor_categories_df.index"
   ]
  },
  {
   "source": [
    "### Create a table which contains the relationships between vendors and their categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 'vendor_id' and 'vendor_category_en' into a new dataframe\n",
    "vendor_category_relationships_df = vendors_df[['vendor_id', 'vendor_category_en']].copy().rename(\n",
    "    columns={'vendor_category_en': 'category'}\n",
    ")\n",
    "\n",
    "# create a dict of 'category' and 'category_id' from 'vendor_categories_df'\n",
    "category_ids = vendor_categories_df.set_index('category')['category_id'].to_dict()\n",
    "\n",
    "# replace the categories with their respective ID\n",
    "vendor_category_relationships_df = vendor_category_relationships_df.replace({'category': category_ids}).rename(\n",
    "    columns={'category': 'category_id'}\n",
    ")\n",
    "\n",
    "# drop the category columns from 'vendors_df'\n",
    "vendors_df = vendors_df.drop(columns=['vendor_category_en', 'vendor_category_id'], axis=1)"
   ]
  },
  {
   "source": [
    "# Extracting Vendor Tags Information\n",
    "\n",
    "### Create a table which contains each tag and an identifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 'vendor_tag_name from 'vendors_df'\n",
    "vendor_tags_df = vendors_df[['vendor_tag_name']].copy().rename(\n",
    "    columns={'vendor_tag_name': 'tag'}\n",
    ")\n",
    "\n",
    "# split tags into lists and explode those lists into new rows for each item\n",
    "vendor_tags_df['tag'] = vendor_tags_df['tag'].str.split(',').tolist()\n",
    "vendor_tags_df = vendor_tags_df.explode('tag').drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "# use the pandas index as 'tag_id'\n",
    "vendor_tags_df['tag_id'] = vendor_tags_df.index"
   ]
  },
  {
   "source": [
    "### Create a table which contains the relationships between vendors and their categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 'vendor_id' and 'vendor_tag_name' from 'vendors_df'\n",
    "vendor_tag_relationships_df = vendors_df[['vendor_id', 'vendor_tag_name']].copy().rename(\n",
    "    columns={'vendor_tag_name': 'tag'}\n",
    ")\n",
    "\n",
    "# explode the lists of tags as before\n",
    "vendor_tag_relationships_df['tag'] = vendor_tag_relationships_df['tag'].str.split(',').tolist()\n",
    "vendor_tag_relationships_df = vendor_tag_relationships_df.explode('tag').reset_index(drop=True)\n",
    "\n",
    "# create a dict of 'tag' and 'tag_id' from 'vendor_tags_df'\n",
    "tag_ids = vendor_tags_df.set_index('tag')['tag_id'].to_dict()\n",
    "\n",
    "# replace the tags with their respective ID\n",
    "vendor_tag_relationships_df = vendor_tag_relationships_df.replace({'tag': tag_ids}).rename(\n",
    "    columns={'tag': 'tag_id'}\n",
    ")\n",
    "\n",
    "# drop the tag columns from 'vendors_df'\n",
    "vendors_df = vendors_df.drop(columns=['vendor_tag', 'vendor_tag_name'], axis=1)"
   ]
  },
  {
   "source": [
    "# Extract Vendor Ratings Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 'vendor_id', 'customer_id', and 'vendor_rating' from 'orders_df'\n",
    "vendor_ratings_df = orders_df[['vendor_id', 'customer_id', 'vendor_rating']].copy().rename(\n",
    "    columns={'vendor_rating': 'rating'}\n",
    ")\n",
    "\n",
    "# remove records with missing 'rating' fields, or those where 'rating' == 0\n",
    "vendor_ratings_df['rating'].replace(0.0, np.nan, inplace=True)\n",
    "vendor_ratings_df = vendor_ratings_df.dropna(subset=['rating']).reset_index(drop=True)\n",
    "\n",
    "# drop 'vendor_rating' from 'orders_df'\n",
    "orders_df = orders_df.drop(['vendor_rating'], axis=1)"
   ]
  },
  {
   "source": [
    "# Extract Promo Codes Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 'promo_code' and 'promo_code_discount_percentage' from 'orders_df'\n",
    "promo_codes_df = orders_df[['promo_code', 'promo_code_discount_percentage']].copy().rename(\n",
    "    columns={'promo_code': 'code', 'promo_code_discount_percentage': 'discount_percentage'}\n",
    ")\n",
    "\n",
    "# convert all codes to lowercase\n",
    "promo_codes_df['code'] = promo_codes_df['code'].str.lower()\n",
    "\n",
    "# remove missing codes and those with missing 'discount_percentage'\n",
    "promo_codes_df['code'].replace('', np.nan, inplace=True)\n",
    "promo_codes_df['discount_percentage'].replace(0.0, np.nan, inplace=True)\n",
    "promo_codes_df = promo_codes_df.dropna(subset=['code', 'discount_percentage'])\n",
    "\n",
    "# remove duplicate codes\n",
    "promo_codes_df = promo_codes_df.drop_duplicates(subset=['code'], keep='first')\n",
    "\n",
    "# use the pandas index as 'code_id'\n",
    "promo_codes_df['code_id'] = promo_codes_df.index\n",
    "\n",
    "# create a dict of 'code' and corresponding 'code_id'\n",
    "code_ids = promo_codes_df.set_index('code')['code_id'].to_dict()\n",
    "\n",
    "# copy new 'code_id' into 'orders_df'\n",
    "orders_df['promo_code'] = orders_df['promo_code'].str.lower()\n",
    "orders_df['promo_code_id'] = orders_df['promo_code'].map(code_ids)\n",
    "\n",
    "# drop 'promo_code' and 'promo_code_discount_percentage' from 'orders_df'\n",
    "orders_df = orders_df.drop(['promo_code', 'promo_code_discount_percentage'], axis=1)"
   ]
  },
  {
   "source": [
    "# Extract Customer Favorite Vendors Information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy 'customer_id' and 'vendor_id' from 'orders_df', where 'is_favorite' == 'Yes'\n",
    "customer_fav_vendors_df = orders_df[orders_df['is_favorite'] == 'Yes'][\n",
    "    ['customer_id', 'vendor_id']\n",
    "].copy().reset_index(drop=True)\n",
    "\n",
    "# drop 'is_favorite' from 'orders_df'\n",
    "orders_df = orders_df.drop(['is_favorite'], axis=1)"
   ]
  },
  {
   "source": [
    "# Finishing Touches\n",
    "## Renaming Order Columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_df.rename(columns={\n",
    "    'item_count': 'n_items', 'grand_total': 'price_due', 'payment_mode': 'payment_method',\n",
    "    'deliverydistance': 'delivery_distance', 'preparationtime': 'preparation_time'\n",
    "})"
   ]
  },
  {
   "source": [
    "## Exporting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.to_csv(\"../data/customers.csv\")\n",
    "vendors_df.to_csv(\"../data/vendors.csv\")\n",
    "locations_df.to_csv(\"../data/locations.csv\")\n",
    "orders_df.to_csv(\"../data/orders.csv\")\n",
    "\n",
    "vendor_categories_df.to_csv(\"../data/vendor_categories.csv\")\n",
    "vendor_category_relationships_df.to_csv(\"../data/vendor_category_relationships.csv\")\n",
    "vendor_tags_df.to_csv(\"../data/vendor_tags.csv\")\n",
    "vendor_tag_relationships_df.to_csv(\"../data/vendor_tag_relationships.csv\")\n",
    "vendor_ratings_df.to_csv(\"../data/vendor_ratings.csv\")\n",
    "promo_codes_df.to_csv(\"../data/promo_codes.csv\")\n",
    "customer_fav_vendors_df.to_csv(\"../data/customer_fav_vendors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}